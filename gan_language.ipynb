{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 16:36:17.125989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alejandrorodriguez/miniconda3/envs/iwgan_tf/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "\n",
    "import language_helpers\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv1d\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Google Billion Word at http://www.statmt.org/lm-benchmark/ and\n",
    "# fill in the path to the extracted files here!\n",
    "DATA_DIR = '/Users/alejandrorodriguez/Documents/1-billion-word-language-modeling-benchmark-r13output'\n",
    "if len(DATA_DIR) == 0:\n",
    "    raise Exception('Please specify path to data directory in gan_language.py!')\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "ITERS = 200000 # How many iterations to train for\n",
    "SEQ_LEN = 32 # Sequence length in characters\n",
    "DIM = 512 # Model dimensionality. This is fairly slow and overfits, even on\n",
    "          # Billion Word. Consider decreasing for smaller datasets.\n",
    "CRITIC_ITERS = 10 # How many critic iterations per generator iteration. We\n",
    "                  # use 10 for the results in the paper, but 5 should work fine\n",
    "                  # as well.\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter.\n",
    "MAX_N_EXAMPLES = 10000000 # Max number of data examples to load. If data loading\n",
    "                          # is too slow or takes too much RAM, you can decrease\n",
    "                          # this (at the expense of having less training data).\n",
    "                          \n",
    "\n",
    "def softmax(logits):\n",
    "    \n",
    "    return tf.reshape(\n",
    "        tf.nn.softmax(\n",
    "            tf.reshape(logits, [-1, len(charmap)])\n",
    "        ),\n",
    "        tf.shape(logits)\n",
    "    )\n",
    "\n",
    "def make_noise(shape):\n",
    "    return tf.random_normal(shape)\n",
    "\n",
    "def ResBlock(name, inputs):\n",
    "\n",
    "    print(f\"ResBlock input dim: {inputs.shape}\")\n",
    "    output = inputs\n",
    "    output = tf.nn.relu(output)\n",
    "    output = lib.ops.conv1d.Conv1D(name+'.1', DIM, DIM, 5, output)\n",
    "    print(f\"ResBlock conv1d 1 out dim: {output.shape}\")\n",
    "    output = tf.nn.relu(output)\n",
    "    print(f\"ResBlock conv1d 2 out dim: {output.shape}\")\n",
    "    output = lib.ops.conv1d.Conv1D(name+'.2', DIM, DIM, 5, output)\n",
    "    return inputs + (0.3*output)\n",
    "\n",
    "def Generator(n_samples, prev_outputs=None):\n",
    "    output = make_noise(shape=[n_samples, 128])\n",
    "    output = lib.ops.linear.Linear('Generator.Input', 128, SEQ_LEN*DIM, output)\n",
    "    output = tf.reshape(output, [-1, DIM, SEQ_LEN])\n",
    "    output = ResBlock('Generator.1', output)\n",
    "    output = ResBlock('Generator.2', output)\n",
    "    output = ResBlock('Generator.3', output)\n",
    "    output = ResBlock('Generator.4', output)\n",
    "    output = ResBlock('Generator.5', output)\n",
    "\n",
    "    print(f\"ResBlock conv1d in dim: {output.shape}\")\n",
    "    output = lib.ops.conv1d.Conv1D('Generator.Output', DIM, len(charmap), 1, output)\n",
    "    \n",
    "    print(f\"ResBlock conv1d out dim: {output.shape}\")\n",
    "    output = tf.transpose(output, [0, 2, 1])\n",
    "    \n",
    "    print(f\"ResBlock transpose out dim: {output.shape}\")\n",
    "    output = softmax(output)\n",
    "    print(f\"ResBlock softmax out dim: {output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    print(f\"Discriminator input dim: {inputs.shape}\")\n",
    "    output = tf.transpose(inputs, [0,2,1])\n",
    "    print(f\"Discriminator transpose out dim: {output.shape}\")\n",
    "    \n",
    "    output = lib.ops.conv1d.Conv1D('Discriminator.Input', len(charmap), DIM, 1, output)\n",
    "    print(f\"Discriminator conv1d out dim: {output.shape}\")\n",
    "    \n",
    "    output = ResBlock('Discriminator.1', output)\n",
    "    output = ResBlock('Discriminator.2', output)\n",
    "    output = ResBlock('Discriminator.3', output)\n",
    "    output = ResBlock('Discriminator.4', output)\n",
    "    output = ResBlock('Discriminator.5', output)\n",
    "    print(f\"Discriminator ResBlock out dim: {output.shape}\")\n",
    "    \n",
    "    output = tf.reshape(output, [-1, SEQ_LEN*DIM])\n",
    "    print(f\"Discriminator reshape out dim: {output.shape}\")\n",
    "    \n",
    "    output = lib.ops.linear.Linear('Discriminator.Output', SEQ_LEN*DIM, 1, output)\n",
    "    print(f\"Discriminator linear out dim: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCRITIC_ITERS: 10\n",
      "\tDATA_DIR: /Users/alejandrorodriguez/Documents/1-billion-word-language-modeling-benchmark-r13output\n",
      "\tDIM: 512\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tMAX_N_EXAMPLES: 10000000\n",
      "\tSEQ_LEN: 32\n",
      "loading dataset...\n",
      "('B', 'u', 't', ' ', 'I', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'p', 'u', 't', ' ', 'm', 'o', 'n', 'e', 'y', ' ', 'o', 'n', ' ', 'a', ' ', 'c', 'o', 'm', 'b', 'i')\n",
      "('\"', ' ', 'I', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', 's', 'i', 'm', 'u', 'l', 't', 'a', 'n', 'e', 'o', 'u', 's', ' ', 'a', 't', 't', 'a', 'c', 'k', ' ', 'a')\n",
      "('S', 'o', 'm', 'e', 'o', 'n', 'e', ' ', 'n', 'e', 'e', 'd', 's', ' ', 't', 'o', ' ', 't', 'e', 'l', 'l', ' ', 'T', 'a', 'r', 'a', ' ', 'R', 'e', 'i', 'd', ' ')\n",
      "('E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', 'd', ' ', 'B', 'r', 'i', 't', 'o', 'n', ' ', 'C', 'h', 'a', 'r', 'l', 'i', 'e', ' ', 'W', 'e', 'g', 'e', 'l')\n",
      "('F', 'r', 'a', 'n', 'k', 'l', 'i', 'n', ' ', \"'\", 's', ' ', 'S', 'o', 'n', 'g', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 'l', 'e', 'n', 't', ' ', 't')\n",
      "('C', 'o', 'a', 'c', 'h', ' ', 'J', 'e', 'f', 'f', ' ', 'F', 'i', 's', 'h', 'e', 'r', ' ', 'a', 'n', 'n', 'o', 'u', 'n', 'c', 'e', 'd', ' ', 't', 'h', 'e', ' ')\n",
      "('\"', ' ', 'I', 'f', ' ', 'I', ' ', 'h', 'a', 'd', ' ', 'a', 'n', 'y', ' ', 'i', 'd', 'e', 'a', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ')\n",
      "('M', 'a', 'i', 'l', 'e', 'r', ' ', ',', ' ', 'w', 'h', 'o', ' ', 'd', 'i', 'e', 'd', ' ', 'S', 'a', 't', 'u', 'r', 'd', 'a', 'y', ' ', 'o', 'f', ' ', 'k', 'i')\n",
      "('T', 'h', 'e', ' ', '\"', ' ', 'W', 'o', 'r', 'l', 'd', ' ', 'B', 'a', 'n', 'k', ' ', 'w', 'a', 's', ' ', 'c', 'r', 'e', 'a', 't', 'e', 'd', ' ', 'n', 'e', 'a')\n",
      "('\"', ' ', 'I', 'n', ' ', 't', 'h', 'e', ' ', 'w', 'e', 'e', 'k', 's', ' ', 'l', 'e', 'a', 'd', 'i', 'n', 'g', ' ', 'u', 'p', ' ', 't', 'o', ' ', 't', 'h', 'e')\n",
      "('I', 'n', ' ', 'a', ' ', 't', 'h', 'i', 'r', 'd', ' ', 'a', 'r', 'e', 'a', ' ', ',', ' ', 't', 'h', 'e', ' ', 'K', 'a', 'n', 'j', 'u', ' ', 'd', 'i', 's', 't')\n",
      "('G', 'u', 'a', 'r', 'd', 'i', 'a', 'n', ' ', 'c', 'o', 'l', 'u', 'm', 'n', 'i', 's', 't', ' ', 'A', 'l', 'e', 'x', 'a', 'n', 'd', 'e', 'r', ' ', 'C', 'h', 'a')\n",
      "('T', 'h', 'e', ' ', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'h', 'a', 'l', 'f', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', '2', '0', 't', 'h', ' ', 'c', 'e', 'n', 't')\n",
      "('T', 'h', 'e', 's', 'e', ' ', 'p', 'e', 'a', 'c', 'e', 'f', 'u', 'l', ' ', 's', 'p', 'a', 'c', 'e', 's', ' ', 'l', 'i', 'e', ' ', 'u', 'n', 'd', 'e', 'r', ' ')\n",
      "('C', 'u', 'r', 'r', 'e', 'n', 't', 'l', 'y', ' ', ',', ' ', 'S', 'e', 'n', 'a', 't', 'e', ' ', 'M', 'a', 'j', 'o', 'r', 'i', 't', 'y', ' ', 'L', 'e', 'a', 'd')\n",
      "('S', 'c', 'h', 'u', 'r', 'i', 'n', 'g', 'a', ' ', 's', 'a', 'i', 'd', ' ', 'h', 'e', ' ', 'n', 'o', 't', 'i', 'c', 'e', 'd', ' ', 'a', ' ', 'm', 'a', 'n', ' ')\n",
      "('T', 'h', 'e', ' ', 'P', 'A', 'T', 'H', ' ', 'R', 'e', 'd', ' ', 't', 'r', 'a', 'i', 'n', ' ', 'a', 'r', 'r', 'i', 'v', 'e', 's', ' ', 'a', 't', ' ', 't', 'h')\n",
      "('T', 'h', 'i', 's', ' ', 'y', 'e', 'a', 'r', ' ', ',', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'h', 'a', 'n', ' ', '9', '7', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't')\n",
      "('M', 'i', 'c', 'h', 'a', 'e', 'l', ' ', 'J', 'r', ' ', '.', ' ', ',', ' ', 'I', ' ', 'l', 'e', 'a', 'r', 'n', 'e', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 'A', 'd')\n",
      "('S', 'o', 'm', 'e', 'd', 'a', 'y', ' ', ',', ' ', 'h', 'e', ' ', \"'\", 's', ' ', 'i', 'n', 't', 'i', 'm', 'a', 't', 'e', 'd', ' ', 'h', 'e', ' ', 'w', 'a', 'n')\n",
      "('A', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'a', 'r', 't', 'i', 'c', 'l', 'e', ' ', 'a', 'p', 'p', 'e', 'a', 'r')\n",
      "('T', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', ',', ' ', 't', 'h', 'o', 's', 'e', ' ', '1', '0', '.', '2', ' ', 'm', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'f', 'o')\n",
      "('T', 'h', 'a', 't', ' ', \"'\", 's', ' ', 'c', 'e', 'r', 't', 'a', 'i', 'n', 'l', 'y', ' ', 't', 'h', 'e', ' ', 'm', 'e', 's', 's', 'a', 'g', 'e', ' ', 'o', 'f')\n",
      "('M', 'o', 'r', 'r', 'i', 's', ' ', 'g', 'r', 'e', 'w', ' ', 'u', 'p', ' ', 'a', ' ', 'h', 'a', 'p', 'p', 'y', ' ', ',', ' ', 's', 'm', 'i', 'l', 'i', 'n', 'g')\n",
      "('T', 'h', 'e', ' ', 'T', 'a', 'r', ' ', 'H', 'e', 'e', 'l', 's', ' ', 'w', 'o', 'n', ' ', 'b', 'y', ' ', '3', '1', ' ', 'p', 'o', 'i', 'n', 't', 's', ' ', 'i')\n",
      "('J', 'o', 'h', 'n', ' ', 'M', 'c', 'C', 'a', 'i', 'n', ' ', ',', ' ', 'w', 'a', 'r', ' ', 'm', 'o', 'n', 'g', 'e', 'r', 'e', 'r', ' ', ';', ' ', 'H', 'i', 'l')\n",
      "('A', ' ', 'm', 'e', 's', 's', 'a', 'g', 'e', ' ', 'l', 'e', 'f', 't', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'M', 'c', 'D', 'o', 'n', 'a', 'l', 'd', ' ', \"'\")\n",
      "('B', 'a', 'n', 's', 'a', 'l', ' ', \"'\", 's', ' ', 's', 'o', 'n', 's', ' ', ',', ' ', 'N', 'i', 'c', 'h', 'o', 'l', 'a', 's', ' ', 'K', 'u', 'm', 'a', 'r', ' ')\n",
      "('W', 'e', ' ', 'b', 'o', 'm', 'b', 'e', 'd', ' ', 'N', 'a', 'g', 'a', 's', 'a', 'k', 'i', ' ', '.', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`')\n",
      "('I', 'n', ' ', 'a', ' ', 'r', 'e', 's', 't', 'l', 'e', 's', 's', ' ', 'd', 'e', 'c', 'a', 'd', 'e', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 't', 'h', 'e', ' ')\n",
      "('T', 'h', 'e', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'n', 'o', 'n', 'c', 'o', 'n', 't', 'r', 'o', 'v', 'e', 'r', 's', 'i', 'a')\n",
      "('T', 'h', 'e', ' ', 'I', 'r', 'a', 'n', 'i', 'a', 'n', ' ', 'g', 'o', 'v', 'e', 'r', 'n', 'm', 'e', 'n', 't', ' ', 'h', 'a', 's', ' ', 'i', 'm', 'p', 'o', 's')\n",
      "('\"', ' ', 'T', 'h', 'e', ' ', '2', '0', ' ', 'y', 'e', 'a', 'r', '-', 'o', 'l', 'd', ' ', 'i', 's', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'g', 'o', 'i', 'n', 'g')\n",
      "('H', 'e', ' ', 'i', 's', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 'm', 'o', 'v', 'e', 'd', ' ', 's', 'o', ' ', 'h', 'i', 's', ' ', 'c', 'o', 'n', 'd', 'i', 't', 'i')\n",
      "('P', 'e', 'r', 's', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'I', ' ', \"'\", 'm', ' ', 'q', 'u', 'i', 't', 'e', ' ', 'p', 'r', 'o', 'u', 'd', ' ', 't', 'h', 'a', 't')\n",
      "('T', 'h', 'e', ' ', 'e', 'v', 'a', 'l', 'u', 'a', 't', 'i', 'o', 'n', 's', ' ', 'a', 'r', 'e', ' ', 'a', ' ', 'f', 'a', 'c', 't', 'o', 'r', ' ', 'i', 'n', ' ')\n",
      "('Q', '.', ' ', 'I', ' ', 'a', 'm', ' ', 't', 'a', 'k', 'i', 'n', 'g', ' ', 'n', 'i', 'a', 'c', 'i', 'n', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 'g', 'h', ' ', 'c')\n",
      "('I', 't', ' ', 'w', 'i', 'l', 'l', ' ', 'm', 'e', 'a', 'n', ' ', 'i', 'm', 'p', 'r', 'o', 'v', 'e', 'd', ' ', 'p', 'i', 'c', 't', 'u', 'r', 'e', ' ', 'q', 'u')\n",
      "('(', ' ', 'O', 'p', 'r', 'a', 'h', '.', 'c', 'o', 'm', ' ', ')', ' ', '-', '-', ' ', 'I', 't', ' ', \"'\", 's', ' ', 'f', 'o', 'u', 'r', ' ', 'd', 'a', 'y', 's')\n",
      "('T', 'h', 'e', ' ', 'd', 'e', 'a', 'd', 'l', 'i', 'e', 's', 't', ' ', 'm', 'i', 'n', 'e', ' ', 'b', 'l', 'a', 's', 't', ' ', 't', 'o', 'o', 'k', ' ', 'p', 'l')\n",
      "('D', 'e', 'm', 'o', 'c', 'r', 'a', 't', 's', ' ', 'a', 'r', 'e', ' ', 's', 't', 'i', 'l', 'l', ' ', 'p', 'o', 'i', 's', 'e', 'd', ' ', 't', 'o', ' ', 'p', 'a')\n",
      "('B', 'u', 't', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'g', 'u', 'r', 'e', ' ', 's', 't', 'a', 'l', 'l', 'e', 'd', ' ', ',', ' ', 'a')\n",
      "('A', 'd', 'j', 'u', 's', 't', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'a', 't', ' ', ',', ' ', 'h', 'e', ' ', 's', 'a', 'i', 'd', ' ', 't', 'h', 'e')\n",
      "('T', 'h', 'e', ' ', 'e', 'm', 'e', 'r', 'g', 'i', 'n', 'g', ' ', 'p', 'o', 'w', 'e', 'r', 's', ' ', ',', ' ', 'l', 'o', 'n', 'g', ' ', 'w', 'a', 'r', 'y', ' ')\n",
      "('\"', ' ', 'P', 'o', 'r', 't', 'u', 'g', 'a', 'l', ' ', 'a', 'n', 'd', ' ', 'A', 'r', 'g', 'e', 'n', 't', 'i', 'n', 'a', ' ', 'a', 'r', 'e', ' ', 's', 't', 'r')\n",
      "('\"', ' ', 'T', 'r', 'o', 'o', 'p', 's', ' ', 'o', 'f', ' ', 'a', 'r', 'm', 'y', ' ', 'T', 'a', 's', 'k', ' ', 'F', 'o', 'r', 'c', 'e', ' ', 'O', 'n', 'e', ' ')\n",
      "('B', 'u', 't', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'w', 'o', 'r', 's', 'e', ' ', ',', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's', 'e', ' ')\n",
      "('B', 'o', 'n', 'u', 's', 'e', 's', ' ', 'r', 'e', 'g', 'a', 'r', 'd', 'l', 'e', 's', 's', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c')\n",
      "('\"', ' ', 'T', 'h', 'e', 'y', ' ', 'g', 'a', 'v', 'e', ' ', 'm', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 'n')\n",
      "('S', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 't', 'h', 'a', 't', ' ', 's', 'h', 'e', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', 'd', ' ', 't', 'h', 'a', 't')\n",
      "('T', 'h', 'e', ' ', 's', 't', 'r', 'i', 'k', 'e', ' ', 'm', 'a', 'r', 'k', 's', ' ', 'a', 'n', ' ', 'e', 's', 'c', 'a', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'b')\n",
      "('A', 'c', 'c', 'o', 'u', 'n', 't', 'a', 'b', 'i', 'l', 'i', 't', 'y', ' ', '?', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`')\n",
      "('A', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'e', ' ', 'c', 'o', 'm', 'p', 'a', 'n', 'y', ' ', ',', ' ', 'A', 'f', 'f', 'i', 'l', 'i', 'a', 't', 'e', 'd', ' ', 'C')\n",
      "('D', 'a', 'v', 'e', ' ', 'W', 'i', 'l', 't', 'o', 'n', ' ', ',', ' ', 'a', 'u', 't', 'h', 'o', 'r', ' ', 'o', 'f', ' ', '\"', ' ', 'W', 'o', 'r', 'd', ' ', 'M')\n",
      "('F', 'r', 'a', 'n', 'k', 'l', 'y', ' ', ',', ' ', 'I', ' ', 'f', 'i', 'n', 'd', ' ', 't', 'h', 'i', 's', ' ', 'i', 'r', 'r', 'e', 'l', 'e', 'v', 'a', 'n', 't')\n",
      "('1', ' ', ',', ' ', '2', '0', '0', '9', ' ', ',', ' ', 'r', 'e', 'p', 'l', 'a', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'U', 'n', 'i', 't', 'e', 'd', ' ')\n",
      "('T', 'h', 'e', ' ', 'C', 'h', 'u', 'r', 'c', 'h', ' ', 'i', 'n', ' ', 'W', 'a', 'l', 'e', 's', ' ', 'p', 'l', 'a', 'c', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'i')\n",
      "('T', 'h', 'i', 's', ' ', 'p', 'r', 'e', 's', 's', ' ', 'r', 'e', 'l', 'e', 'a', 's', 'e', ' ', 'i', 'n', 'c', 'l', 'u', 'd', 'e', 's', ' ', '\"', ' ', 'f', 'o')\n",
      "('T', 'h', 'e', ' ', 'H', 'o', 'n', 'd', 'a', ' ', 'r', 'i', 'd', 'e', 'r', ' ', 'f', 'l', 'e', 'w', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'U', 'n', 'i', 't')\n",
      "('T', 'h', 'e', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'p', 'r', 'e', 't', 't', 'y', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'a', 's', 't', 'e', 's', ' ', 'W', 'a', 't')\n",
      "('D', 'e', 's', 'p', 'i', 't', 'e', ' ', 't', 'h', 'e', ' ', 's', 't', 'o', 'r', 'i', 'e', 's', ' ', ',', ' ', 'h', 'o', 'w', 'e', 'v', 'e', 'r', ' ', ',', ' ')\n",
      "('A', 'N', 'D', ' ', 'H', 'E', ' ', 'K', 'N', 'O', 'W', 'S', ' ', 'I', 'T', ' ', '!', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`')\n",
      "('S', 'o', 'm', 'e', ' ', 'a', 'n', 'a', 'l', 'y', 's', 't', 's', ' ', 's', 'a', 'i', 'd', ' ', 'h', 'i', 's', ' ', 'd', 'e', 'p', 'a', 'r', 't', 'u', 'r', 'e')\n",
      "('T', 'h', 'e', ' ', 'k', 'e', 'y', ' ', 'i', 's', ' ', 'l', 'u', 'r', 'i', 'n', 'g', ' ', 't', 'h', 'o', 's', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'o')\n",
      "('T', 'h', 'i', 'n', 'k', ' ', 'o', 'f', ' ', 'p', 'r', 'e', 'v', 'i', 'o', 'u', 's', ' ', 'w', 'i', 'n', 'n', 'e', 'r', 's', ' ', 'T', 'i', 'm', ' ', ',', ' ')\n",
      "('H', 'e', ' ', 'l', 'i', 'v', 'e', 's', ' ', 'i', 'n', ' ', 'L', 'e', 'u', 't', 'e', 'n', 'b', 'a', 'c', 'h', ' ', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'K')\n",
      "('T', 'h', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 'l', ' ', 'h', 'a', 's', ' ', 't', 'h', 'r', 'e', 'a', 't', 'e', 'n', 'e', 'd', ' ', 'v', 'i', 'o', 'l', 'e')\n",
      "('C', 'B', 'S', ' ', 'T', 'e', 'l', 'e', 'v', 'i', 's', 'i', 'o', 'n', ' ', 'D', 'i', 's', 't', 'r', 'i', 'b', 'u', 't', 'i', 'o', 'n', ' ', ',', ' ', 'w', 'h')\n",
      "('D', 'o', 'c', 'u', 'm', 'e', 'n', 't', 's', ' ', 'f', 'i', 'l', 'e', 'd', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'S', 't', '.', ' ', 'C', 'l', 'a', 'i', 'r')\n",
      "('T', 'w', 'o', ' ', 's', 't', 'r', 'e', 'e', 't', ' ', 'b', 'o', 'm', 'b', 'i', 'n', 'g', 's', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 'm', 'o', 'r', 't', 'a', 'r')\n",
      "('T', 'e', 'm', 'a', 's', 'e', 'k', ' ', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'a', 's', 't', ' ', 'y', 'e')\n",
      "('I', 'n', 'd', 'e', 'e', 'd', ' ', ',', ' ', 'h', 'e', ' ', 's', 'a', 'i', 'd', ' ', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'h', 'a', 'p', 'p', 'y', ' ', 't', 'o')\n",
      "('\"', ' ', 'A', 'n', 'y', 'b', 'o', 'd', 'y', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'r')\n",
      "('A', 'b', 'o', 'u', 't', ' ', '5', '0', ' ', 'p', 'a', 't', 'i', 'e', 'n', 't', 's', ' ', 'r', 'e', 'c', 'e', 'i', 'v', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'd')\n",
      "('A', ' ', 'n', 'e', 'w', ' ', 'h', 'i', 'g', 'h', '-', 's', 'p', 'e', 'e', 'd', ' ', 'u', 'n', 'd', 'e', 'r', 's', 'e', 'a', ' ', 'c', 'a', 'b', 'l', 'e', ' ')\n",
      "('T', 'h', 'e', 'y', ' ', 'f', 'i', 'n', 'i', 's', 'h', 'e', 'd', ' ', '4', '-', '1', '2', ' ', '.', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`', '`')\n",
      "('T', 'h', 'e', ' ', 'a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'n', ' ', 'c', 'a', 'u', 's', 'e', ' ', 'b', 'i')\n",
      "('A', 't', ' ', 't', 'h', 'e', ' ', 'G', 'e', 'o', 'r', 'g', 'i', 'a', ' ', 'A', 'v', 'e', 'n', 'u', 'e', '-', 'P', 'e', 't', 'w', 'o', 'r', 't', 'h', ' ', 's')\n",
      "('C', 'e', 'l', 'e', 'b', 'r', 'i', 't', 'y', ' ', 't', 'u', 'r', 'n', 'e', 'd', ' ', 'd', 'e', 's', 'i', 'g', 'n', 'e', 'r', ' ', 'a', 'c', 't', 'u', 'a', 'l')\n",
      "('A', 's', ' ', 'h', 'e', ' ', 'l', 'e', 'a', 'd', 's', ' ', 'u', 's', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 'h', 'i', 's', ' ', 'o', 'r', 'c', 'h', 'a')\n",
      "('F', 'o', 'r', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'e', 'a', 's', 'u', 'r', 'e', ' ', 'M', 'u', 'r', 'r', 'a', 'y', ' ', 'w', 'i', 'l', 'l')\n",
      "('I', 'n', ' ', '2', '0', '0', '3', ' ', ',', ' ', 'H', 'u', 'c', 'k', 'a', 'b', 'e', 'e', ' ', 's', 'a', 'i', 'd', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e')\n",
      "('I', 't', ' ', 'i', 's', ' ', 'k', 'n', 'o', 'w', 'n', ' ', 't', 'h', 'a', 't', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 's', 't', 'r', 'e', 's', 's', 'e', 'd', ' ')\n",
      "('T', 'h', 'e', ' ', 'A', 'r', 'i', 'z', 'o', 'n', 'a', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', ' ', 'u', 'n', 'd', 'e', 'r', 's', 'c', 'o', 'r', 'e', 's', ' ')\n",
      "('G', 'u', ' ', 'a', 'n', 'd', ' ', 'h', 'e', 'r', ' ', 'h', 'u', 's', 'b', 'a', 'n', 'd', ' ', 's', 'p', 'e', 'n', 't', ' ', '6', '0', '0', ' ', 'y', 'u', 'a')\n",
      "('S', 't', 'o', 'k', 'e', ' ', ':', ' ', 'S', 'o', 'r', 'e', 'n', 's', 'e', 'n', ' ', ',', ' ', 'G', 'r', 'i', 'f', 'f', 'i', 'n', ' ', ',', ' ', 'A', 'b', 'd')\n",
      "('T', 'h', 'e', ' ', 'a', 'c', 't', 'i', 'v', 'i', 's', 't', 's', ' ', 'a', 'c', 'k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e', 'd', ' ', 't', 'a', 'k', 'i', 'n')\n",
      "('A', 'g', 'e', 'n', 't', ' ', 'A', 'l', 'a', 'i', 'n', ' ', 'M', 'i', 'g', 'l', 'i', 'a', 'c', 'c', 'i', 'o', ' ', 't', 'o', 'l', 'd', ' ', 'B', 'i', 'l', 'd')\n",
      "('M', 'a', 't', 't', ' ', 'M', 'i', 'l', 'l', 's', ' ', 'a', 'l', 's', 'o', ' ', 'g', 'l', 'a', 'n', 'c', 'e', 'd', ' ', 'w', 'i', 'd', 'e', ' ', 'a', ' ', 'c')\n",
      "('S', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r')\n",
      "('P', 'o', 'l', 'i', 't', 'i', 'c', 'a', 'l', ' ', 'a', 'n', 'a', 'l', 'y', 's', 't', 's', ' ', 's', 'a', 'y', ' ', 'M', 'r', ' ', 'T', 'h', 'a', 'k', 's', 'i')\n",
      "('H', 'o', 'w', 'e', 'v', 'e', 'r', ' ', ',', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 's', ' ', 'p', 'e', 'r', 's', 'i', 's', 't', ' ', 'a', 't', ' ', 't', 'h')\n",
      "('M', 'r', ' ', 'A', 'l', 'a', 'n', ' ', 'M', 'u', 'h', 'a', 'm', 'm', 'e', 'd', ' ', 'r', 'e', 's', 'i', 'g', 'n', 'e', 'd', ' ', 'a', 's', ' ', 'G', 'u', 'i')\n",
      "('B', 'e', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'j', 'a', 'c', 'k', 'p', 'o', 't', ' ', ',', ' ', 'p', 'r', 'i', 'z', 'e', 's', ' ', 'r', 'a', 'n')\n",
      "('P', 'A', 'R', 'I', 'S', ' ', '-', ' ', 'A', ' ', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'a', 'c', 'c', 'i', 'd', 'e', 'n', 't', ' ', 'i', 'n', 'v', 'e', 's', 't')\n",
      "('C', 'l', 'y', 'd', 'e', ' ', 'm', 'a', 'n', 'a', 'g', 'e', 'r', ' ', 'J', 'o', 'h', 'n', ' ', 'B', 'r', 'o', 'w', 'n', ' ', 'h', 'a', 's', ' ', 's', 'n', 'a')\n",
      "('T', 'o', ' ', 'k', 'e', 'e', 'p', ' ', 't', 'h', 'e', 'm', ' ', 'i', 'n', ' ', 's', 'y', 'n', 'c', ' ', '\"', ' ', 'l', 'e', 'a', 'p', ' ', 's', 'e', 'c', 'o')\n",
      "('K', 'a', 'z', 'e', 'm', 'i', ' ', 'w', 'a', 's', ' ', 'w', 'o', 'r', 'k', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'w', 'a', 'i', 't', 'r', 'e', 's', 's')\n",
      "('B', 'e', 't', 'w', 'e', 'e', 'n', ' ', '5', '0', ' ', 'a', 'n', 'd', ' ', '1', '0', '0', ' ', 'S', 'u', 'n', 'n', 'i', ' ', 'f', 'a', 'm', 'i', 'l', 'i', 'e')\n",
      "('T', 'h', 'e', 'i', 'r', ' ', 't', 'o', 'p', ' ', 't', 'w', 'o', ' ', 'l', 'e', 'a', 'd', 'e', 'r', 's', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h')\n",
      "loaded 10000000 lines in dataset\n"
     ]
    }
   ],
   "source": [
    "lib.print_model_settings(locals().copy())\n",
    "\n",
    "lines, charmap, inv_charmap = language_helpers.load_dataset(\n",
    "    max_length=SEQ_LEN,\n",
    "    max_n_examples=MAX_N_EXAMPLES,\n",
    "    data_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alejandrorodriguez/miniconda3/envs/iwgan_tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NCHW is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NCHW` for data_format is deprecated, use `NCW` instead\n"
     ]
    }
   ],
   "source": [
    "real_inputs_discrete = tf.placeholder(tf.int32, shape=[BATCH_SIZE, SEQ_LEN])\n",
    "real_inputs = tf.one_hot(real_inputs_discrete, len(charmap))\n",
    "fake_inputs = Generator(BATCH_SIZE)\n",
    "fake_inputs_discrete = tf.argmax(fake_inputs, fake_inputs.get_shape().ndims-1)\n",
    "\n",
    "print(f\"real_inputs_discrete dim: {real_inputs_discrete.shape}\")\n",
    "print(f\"real_inputs dim: {real_inputs.shape}\")\n",
    "print(f\"fake_inputs dim: {fake_inputs.shape}\")\n",
    "print(f\"fake_inputs_discrete dim: {fake_inputs_discrete.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = Discriminator(real_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator input dim: (64, 32, 297)\n",
      "Discriminator transpose out dim: (64, 297, 32)\n",
      "Discriminator conv1d out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "Discriminator ResBlock out dim: (64, 512, 32)\n",
      "Discriminator reshape out dim: (64, 16384)\n",
      "Discriminator linear out dim: (64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_57:0' shape=(64, 32, 297) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "disc_fake = Discriminator(fake_inputs)\n",
    "\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "\n",
    "# WGAN lipschitz-penalty\n",
    "alpha = tf.random_uniform(\n",
    "    shape=[BATCH_SIZE,1,1], \n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "differences = fake_inputs - real_inputs\n",
    "interpolates = real_inputs + (alpha*differences)\n",
    "interpolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Discriminator.Output_5/BiasAdd:0' shape=(64, 1) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator(interpolates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_57:0' shape=(64, 32, 297) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCRITIC_ITERS: 10\n",
      "\tDATA_DIR: /Users/alejandrorodriguez/Documents/1-billion-word-language-modeling-benchmark-r13output\n",
      "\tDIM: 512\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tMAX_N_EXAMPLES: 10000000\n",
      "\tSEQ_LEN: 32\n"
     ]
    }
   ],
   "source": [
    "lib.print_model_settings(locals().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator input dim: (64, 32, 297)\n",
      "Discriminator transpose out dim: (64, 297, 32)\n",
      "Discriminator conv1d out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "ResBlock input dim: (64, 512, 32)\n",
      "ResBlock conv1d 1 out dim: (64, 512, 32)\n",
      "ResBlock conv1d 2 out dim: (64, 512, 32)\n",
      "Discriminator ResBlock out dim: (64, 512, 32)\n",
      "Discriminator reshape out dim: (64, 16384)\n",
      "Discriminator linear out dim: (64, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "\n",
    "# WGAN lipschitz-penalty\n",
    "alpha = tf.random_uniform(\n",
    "    shape=[BATCH_SIZE,1,1], \n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "differences = fake_inputs - real_inputs\n",
    "interpolates = real_inputs + (alpha*differences)\n",
    "gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1,2]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "disc_cost += LAMBDA*gradient_penalty\n",
    "gen_params = lib.params_with_name('Generator')\n",
    "disc_params = lib.params_with_name('Discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set JSD for n=1: 0.0009669324514356473\n",
      "validation set JSD for n=2: 0.01567580406923486\n",
      "validation set JSD for n=3: 0.0885953191307561\n",
      "validation set JSD for n=4: 0.2454666793886078\n",
      "WARNING:tensorflow:From /Users/alejandrorodriguez/miniconda3/envs/iwgan_tf/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 19:08:00.612693: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-22 19:08:00.613051: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-01-22 19:08:03.032571: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# Train critic\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m#for i in xrange(CRITIC_ITERS):\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(CRITIC_ITERS):\n\u001b[0;32m---> 58\u001b[0m     _data \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49mnext()\n\u001b[1;32m     59\u001b[0m     _disc_cost, _ \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mrun(\n\u001b[1;32m     60\u001b[0m         [disc_cost, disc_train_op],\n\u001b[1;32m     61\u001b[0m         feed_dict\u001b[39m=\u001b[39m{real_inputs_discrete:_data}\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     64\u001b[0m lib\u001b[39m.\u001b[39mplot\u001b[39m.\u001b[39mplot(\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(gen_cost, var_list=gen_params)\n",
    "disc_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(disc_cost, var_list=disc_params)\n",
    "\n",
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        np.random.shuffle(lines)\n",
    "        #for i in xrange(0, len(lines)-BATCH_SIZE+1, BATCH_SIZE):\n",
    "        for i in range(0, len(lines)-BATCH_SIZE+1, BATCH_SIZE):\n",
    "            yield np.array(\n",
    "                [[charmap[c] for c in l] for l in lines[i:i+BATCH_SIZE]], \n",
    "                dtype='int32'\n",
    "            )\n",
    "\n",
    "# During training we monitor JS divergence between the true & generated ngram\n",
    "# distributions for n=1,2,3,4. To get an idea of the optimal values, we\n",
    "# evaluate these statistics on a held-out set first.\n",
    "#true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[10*BATCH_SIZE:], tokenize=False) for i in xrange(4)]\n",
    "#validation_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[:10*BATCH_SIZE], tokenize=False) for i in xrange(4)]\n",
    "true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[10*BATCH_SIZE:], tokenize=False) for i in range(4)]\n",
    "validation_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[:10*BATCH_SIZE], tokenize=False) for i in range(4)]\n",
    "#for i in xrange(4):\n",
    "for i in range(4):\n",
    "    print(\"validation set JSD for n={}: {}\".format(i+1, true_char_ngram_lms[i].js_with(validation_char_ngram_lms[i])))\n",
    "#true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines, tokenize=False) for i in xrange(4)]\n",
    "true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines, tokenize=False) for i in range(4)]\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    def generate_samples():\n",
    "        samples = session.run(fake_inputs)\n",
    "        samples = np.argmax(samples, axis=2)\n",
    "        decoded_samples = []\n",
    "        #for i in xrange(len(samples)):\n",
    "        for i in range(len(samples)):\n",
    "            decoded = []\n",
    "            #for j in xrange(len(samples[i])):\n",
    "            for j in range(len(samples[i])):\n",
    "                decoded.append(inv_charmap[samples[i][j]])\n",
    "            decoded_samples.append(tuple(decoded))\n",
    "        return decoded_samples\n",
    "\n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    #for iteration in xrange(ITERS):\n",
    "    for iteration in range(ITERS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train generator\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "\n",
    "        # Train critic\n",
    "        #for i in xrange(CRITIC_ITERS):\n",
    "        for i in range(CRITIC_ITERS):\n",
    "            _data = gen.next()\n",
    "            _disc_cost, _ = session.run(\n",
    "                [disc_cost, disc_train_op],\n",
    "                feed_dict={real_inputs_discrete:_data}\n",
    "            )\n",
    "\n",
    "        lib.plot.plot('time', time.time() - start_time)\n",
    "        lib.plot.plot('train disc cost', _disc_cost)\n",
    "\n",
    "        if iteration % 100 == 99:\n",
    "            samples = []\n",
    "            #for i in xrange(10):\n",
    "            for i in range(10):\n",
    "                samples.extend(generate_samples())\n",
    "\n",
    "            #for i in xrange(4):\n",
    "            for i in range(4):\n",
    "                lm = language_helpers.NgramLanguageModel(i+1, samples, tokenize=False)\n",
    "                lib.plot.plot('js{}'.format(i+1), lm.js_with(true_char_ngram_lms[i]))\n",
    "\n",
    "            with open('samples_{}.txt'.format(iteration), 'w') as f:\n",
    "                for s in samples:\n",
    "                    s = \"\".join(s)\n",
    "                    f.write(s + \"\\n\")\n",
    "\n",
    "        if iteration % 100 == 99:\n",
    "            lib.plot.flush()\n",
    "        \n",
    "        lib.plot.tick()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iwgan_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7505e247c96423b10181be14adfe543eb7307c4b3977c6293179173905ce303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
